# Nano-Fractal 4B

> Emergent Fractal Intelligence: A hybrid Mamba-Transformer with dynamic Matryoshka scaling.

**Fork of [karpathy/nanochat](https://github.com/karpathy/nanochat)** â€” Extended with:

- ğŸ§¬ **Hybrid Architecture**: 32 Attention + 32 Mamba layers (64 total, ~4B params)
- ğŸ“ **Matryoshka Dimensions**: Ghost (128) â†’ God (4096) dynamic scaling
- ğŸ§  **Neural Confidence Probes**: Per-layer topological signals for capacity allocation
- âš¡ **Energy Penalty Loss**: Forces compute minimization ("lazy but correct")

## Architecture

```
nanochat-d32 (1.9B) â†’ Surgery â†’ Nano-Fractal 4B
â”œâ”€â”€ 32 Attention layers (pretrained, expanded 2048â†’4096)
â”œâ”€â”€ 32 Mamba layers (new, interleaved)
â”œâ”€â”€ MatryoshkaMLP: [128, 512, 1024, 2048, 4096]
â”œâ”€â”€ MatryoshkaKV: [32, 64, 128, 256]
â””â”€â”€ ConfidenceProbe per layer (variance, agreement, spread)
```

## New Files

| File | Purpose |
|------|---------|
| `nanochat/mamba_block.py` | Mamba layer with SSM fallback |
| `nanochat/confidence_probe.py` | Neural probes using topological signals |
| `nanochat/matryoshka.py` | Dimension slicing + energy penalty |
| `nanochat/hybrid_gpt.py` | HybridGPT (interleaved Mamba+Attention) |
| `scripts/surgery.py` | Convert nanochat-d32 â†’ hybrid |
| `scripts/fractal_train.py` | Training with Matryoshka loss |

## Quick Start

```bash
# 1. Download base model
huggingface-cli download karpathy/nanochat-d32 --local-dir ~/.cache/nanochat/chatsft_checkpoints/d32

# 2. Run surgery (creates 4B hybrid)
python -m scripts.surgery

# 3. Train on 8Ã—H100
torchrun --nproc_per_node=8 -m scripts.fractal_train \
    --checkpoint ~/.cache/nanochat/hybrid_checkpoints/d32/model_surgery.pt \
    --matryoshka --sample-dim \
    --num-iterations=5000
```

## Ghost â†’ God Spectrum

| Mode | MLP Dim | KV Dim | Compute |
|------|---------|--------|---------|
| Ghost | 128 | 32 | ~0.1% |
| Whisper | 512 | 64 | ~1.5% |
| Normal | 2048 | 128 | ~25% |
| Think | 4096 | 256 | ~100% |

The model learns *when* to scale up based on task difficulty.

## Training Cost

~$200 total on 8Ã—H100 (~13 hours):
- Phase 1: Expand + Initialize (~2 hrs)
- Phase 2: Matryoshka Training (~6 hrs)  
- Phase 3: Emergent Think Training (~3 hrs)

---

*Based on [nanochat](https://github.com/karpathy/nanochat) by Andrej Karpathy*
